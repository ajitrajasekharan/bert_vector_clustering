{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers sacremoses"
      ],
      "metadata": {
        "id": "m0RsXkue3W-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "import pdb\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import traceback\n",
        "import argparse\n",
        "import string\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "#DEFAULT_MODEL_PATH='bert-large-cased'\n",
        "#DEFAULT_MODEL_PATH='bert-base-cased' #works best for names\n",
        "#DEFAULT_MODEL_PATH='bert-base-uncased'\n",
        "DEFAULT_MODEL_PATH='./'\n",
        "DEFAULT_TO_LOWER=False\n",
        "DEFAULT_TOP_K = 20\n",
        "ACCRUE_THRESHOLD = 1\n",
        "\n",
        "def init_model(model_path,to_lower):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    print(\"******* MODEL[path] is:\",model_path,\" lower casing is set to:\",to_lower)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
        "    model = BertForMaskedLM.from_pretrained(model_path)\n",
        "    #tokenizer = RobertaTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
        "    #model = RobertaForMaskedLM.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    return model,tokenizer\n",
        "\n",
        "def get_sent(to_lower):\n",
        "    print(\"Enter sentence (optionally including the special token [MASK] if tolower is set to False). Type q to quit\")\n",
        "    sent = input()\n",
        "    if (sent == 'q'):\n",
        "        return sent\n",
        "    else:\n",
        "        return  sent.lower() if to_lower else sent\n",
        "\n",
        "def read_descs(file_name):\n",
        "    ret_dict = {}\n",
        "    with open(file_name) as fp:\n",
        "        line = fp.readline().rstrip(\"\\n\")\n",
        "        if (len(line) >= 1):\n",
        "            ret_dict[line] = 1\n",
        "        while line:\n",
        "            line = fp.readline().rstrip(\"\\n\")\n",
        "            if (len(line) >= 1):\n",
        "                ret_dict[line] = 1\n",
        "    return ret_dict\n",
        "\n",
        "\n",
        "\n",
        "def perform_task(model,tokenizer,top_k,accrue_threshold,text,patched):\n",
        "    text = '[CLS] ' + text + ' [SEP]' \n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # Create the segments tensors.\n",
        "    segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "    print(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(tokens_tensor, segments_tensors)\n",
        "        for i in range(len(tokenized_text)):\n",
        "                #if (i != 0 and i != len(tokenized_text) - 1):\n",
        "                #    continue\n",
        "                results_dict = {}\n",
        "                masked_index = i\n",
        "                neighs_dict = {}\n",
        "                if (patched):\n",
        "                    for j in range(len(predictions[0][0][0,masked_index])):\n",
        "                        if (float(predictions[0][0][0,masked_index][j].tolist()) > accrue_threshold):\n",
        "                            tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                            results_dict[tok] = float(predictions[0][0][0,masked_index][j].tolist())\n",
        "                        tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                        if (tok in tokenized_text):\n",
        "                            neighs_dict[tok] = float(predictions[0][0][0,masked_index][j].tolist())\n",
        "                else:\n",
        "                    for j in range(len(predictions[0][0][masked_index])):\n",
        "                        if (float(predictions[0][0][masked_index][j].tolist()) > accrue_threshold):\n",
        "                            tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                            results_dict[tok] = float(predictions[0][0][masked_index][j].tolist())\n",
        "                        tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                        if (tok in tokenized_text):\n",
        "                            neighs_dict[tok] = float(predictions[0][0][masked_index][j].tolist())\n",
        "                k = 0\n",
        "                sorted_d = OrderedDict(sorted(results_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                print(\"********* Top predictions for token: \",tokenized_text[i])\n",
        "                for index in sorted_d:\n",
        "                    if (index in string.punctuation or index.startswith('##') or len(index) == 1 or index.startswith('.') or index.startswith('[')):\n",
        "                        continue\n",
        "                    print(index,round(float(sorted_d[index]),4))\n",
        "                    k += 1\n",
        "                    if (k > top_k):\n",
        "                        break\n",
        "                print(\"********* Closest sentence neighbors in output to the token :  \",tokenized_text[i])\n",
        "                sorted_d = OrderedDict(sorted(neighs_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                for index in sorted_d:\n",
        "                    if (index in string.punctuation or index.startswith('##') or len(index) == 1 or index.startswith('.') or index.startswith('[')):\n",
        "                        continue\n",
        "                    print(index,round(float(sorted_d[index]),4))\n",
        "                print()\n",
        "                print()\n",
        "                #break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = \"bert-base-cased\"\n",
        "topk = DEFAULT_TOP_K\n",
        "tolower=False\n",
        "patched = False\n",
        "threshold = ACCRUE_THRESHOLD\n",
        "model,tokenizer = init_model(model,tolower)\n",
        "text = \"Aziz Khan is an [MASK]\"\n",
        "perform_task(model,tokenizer,topk,threshold,text,patched)"
      ],
      "metadata": {
        "id": "ufTuOUHi3KQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "test_notebook.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}