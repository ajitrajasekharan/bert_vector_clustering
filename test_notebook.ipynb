{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers sacremoses"
      ],
      "metadata": {
        "id": "m0RsXkue3W-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "import pdb\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import traceback\n",
        "import argparse\n",
        "import string\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "#DEFAULT_MODEL_PATH='bert-large-cased'\n",
        "#DEFAULT_MODEL_PATH='bert-base-cased' #works best for names\n",
        "#DEFAULT_MODEL_PATH='bert-base-uncased'\n",
        "DEFAULT_MODEL_PATH='./'\n",
        "DEFAULT_TO_LOWER=False\n",
        "DEFAULT_TOP_K = 20\n",
        "ACCRUE_THRESHOLD = 1\n",
        "\n",
        "def init_model(model_path,to_lower):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    print(\"******* MODEL[path] is:\",model_path,\" lower casing is set to:\",to_lower)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
        "    model = BertForMaskedLM.from_pretrained(model_path)\n",
        "    #tokenizer = RobertaTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
        "    #model = RobertaForMaskedLM.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    return model,tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_task(model,tokenizer,top_k,accrue_threshold,text,patched):\n",
        "    text = '[CLS] ' + text + ' [SEP]' \n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # Create the segments tensors.\n",
        "    segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "    print(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(tokens_tensor, segments_tensors)\n",
        "        for i in range(len(tokenized_text)):\n",
        "                #if (i != 0 and i != len(tokenized_text) - 1):\n",
        "                #    continue\n",
        "                results_dict = {}\n",
        "                masked_index = i\n",
        "                neighs_dict = {}\n",
        "                if (patched):\n",
        "                    for j in range(len(predictions[0][0][0,masked_index])):\n",
        "                        if (float(predictions[0][0][0,masked_index][j].tolist()) > accrue_threshold):\n",
        "                            tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                            results_dict[tok] = float(predictions[0][0][0,masked_index][j].tolist())\n",
        "                        tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                        if (tok in tokenized_text):\n",
        "                            neighs_dict[tok] = float(predictions[0][0][0,masked_index][j].tolist())\n",
        "                else:\n",
        "                    for j in range(len(predictions[0][0][masked_index])):\n",
        "                        if (float(predictions[0][0][masked_index][j].tolist()) > accrue_threshold):\n",
        "                            tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                            results_dict[tok] = float(predictions[0][0][masked_index][j].tolist())\n",
        "                        tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                        if (tok in tokenized_text):\n",
        "                            neighs_dict[tok] = float(predictions[0][0][masked_index][j].tolist())\n",
        "                k = 0\n",
        "                sorted_d = OrderedDict(sorted(results_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                print(\"********* Top predictions for token: \",tokenized_text[i])\n",
        "                for index in sorted_d:\n",
        "                    if (index in string.punctuation or index.startswith('##') or len(index) == 1 or index.startswith('.') or index.startswith('[')):\n",
        "                        continue\n",
        "                    print(index,round(float(sorted_d[index]),4))\n",
        "                    k += 1\n",
        "                    if (k > top_k):\n",
        "                        break\n",
        "                print(\"********* Closest sentence neighbors in output to the token :  \",tokenized_text[i])\n",
        "                sorted_d = OrderedDict(sorted(neighs_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                for index in sorted_d:\n",
        "                    if (index in string.punctuation or index.startswith('##') or len(index) == 1 or index.startswith('.') or index.startswith('[')):\n",
        "                        continue\n",
        "                    print(index,round(float(sorted_d[index]),4))\n",
        "                print()\n",
        "                print()\n",
        "                #break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ufTuOUHi3KQO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modiy params here"
      ],
      "metadata": {
        "id": "VkrTX3mc7fik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = \"bert-base-cased\"\n",
        "topk = DEFAULT_TOP_K\n",
        "tolower=False\n",
        "patched = False\n",
        "threshold = ACCRUE_THRESHOLD\n",
        "model,tokenizer = init_model(model,tolower)\n",
        "text = \"Abubakkar Siddiq is a [MASK]\"\n",
        "perform_task(model,tokenizer,topk,threshold,text,patched)"
      ],
      "metadata": {
        "id": "q6H8--aO7iIz",
        "outputId": "ea89217e-873d-4232-c398-151483f6d9e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******* MODEL[path] is: bert-base-cased  lower casing is set to: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Abu', '##ba', '##kka', '##r', 'Sid', '##di', '##q', 'is', 'a', '[MASK]', '[SEP]']\n",
            "********* Top predictions for token:  [CLS]\n",
            "the 4.6188\n",
            "and 4.2249\n",
            "of 4.2042\n",
            "to 3.9931\n",
            "in 3.9864\n",
            "on 3.1076\n",
            "was 3.077\n",
            "him 3.049\n",
            "it 3.0144\n",
            "for 2.9258\n",
            "her 2.9244\n",
            "as 2.8541\n",
            "that 2.8447\n",
            "families 2.7784\n",
            "with 2.7692\n",
            "me 2.6936\n",
            "is 2.6631\n",
            "he 2.6377\n",
            "at 2.6315\n",
            "by 2.5921\n",
            "his 2.5727\n",
            "********* Closest sentence neighbors in output to the token :   [CLS]\n",
            "is 2.6631\n",
            "Sid -2.5124\n",
            "Abu -2.5838\n",
            "\n",
            "\n",
            "********* Top predictions for token:  Abu\n",
            "Abu 14.6061\n",
            "Abdul 8.112\n",
            "Mu 7.8512\n",
            "El 6.2221\n",
            "In 6.1822\n",
            "Ta 5.9992\n",
            "in 5.8125\n",
            "the 5.6926\n",
            "Abd 5.5745\n",
            "and 5.5084\n",
            "of 5.3902\n",
            "to 5.1345\n",
            "As 5.041\n",
            "Ahmed 5.0399\n",
            "Baba 4.978\n",
            "Ibrahim 4.8955\n",
            "Fu 4.8829\n",
            "The 4.7856\n",
            "Allah 4.7632\n",
            "Do 4.6964\n",
            "Wu 4.6401\n",
            "********* Closest sentence neighbors in output to the token :   Abu\n",
            "Abu 14.6061\n",
            "is 3.7958\n",
            "Sid 1.316\n",
            "\n",
            "\n",
            "********* Top predictions for token:  ##ba\n",
            "Ba 19.1697\n",
            "Ma 13.368\n",
            "Baba 12.8254\n",
            "Bo 11.208\n",
            "Dana 11.167\n",
            "Bey 10.0911\n",
            "ma 9.9483\n",
            "Na 9.8936\n",
            "Mira 8.7935\n",
            "Da 8.6862\n",
            "Bar 8.6612\n",
            "Ta 8.6009\n",
            "ta 8.5629\n",
            "Sa 8.4366\n",
            "Maya 8.106\n",
            "Jam 8.0206\n",
            "Bee 7.927\n",
            "Bang 7.8674\n",
            "Bob 7.8624\n",
            "Bay 7.711\n",
            "Ha 7.6995\n",
            "********* Closest sentence neighbors in output to the token :   ##ba\n",
            "Abu 7.4927\n",
            "is 3.5444\n",
            "Sid -2.1837\n",
            "\n",
            "\n",
            "********* Top predictions for token:  ##kka\n",
            "Ka 12.9566\n",
            "Ku 9.6906\n",
            "ka 9.1896\n",
            "Ko 8.7452\n",
            "Sega 8.0614\n",
            "Kara 7.8377\n",
            "Sami 7.7494\n",
            "Ke 7.7257\n",
            "Omar 7.1432\n",
            "Kay 7.0708\n",
            "Grove 6.934\n",
            "Ham 6.9204\n",
            "near 6.7797\n",
            "Mei 6.755\n",
            "Kenyan 6.7261\n",
            "Lima 6.7003\n",
            "Cape 6.6506\n",
            "bike 6.5168\n",
            "Malaysian 6.4847\n",
            "Sadie 6.4575\n",
            "Mega 6.4436\n",
            "********* Closest sentence neighbors in output to the token :   ##kka\n",
            "is 2.3444\n",
            "Abu 1.9602\n",
            "Sid -1.8944\n",
            "\n",
            "\n",
            "********* Top predictions for token:  ##r\n",
            "Muhammad 8.801\n",
            "Mirza 8.7708\n",
            "and 8.7167\n",
            "Mohammed 8.644\n",
            "Abdul 8.6166\n",
            "bin 8.4111\n",
            "Abdullah 8.3969\n",
            "Amir 8.3667\n",
            "of 8.0069\n",
            "Mohammad 7.7664\n",
            "or 7.6852\n",
            "man 7.0919\n",
            "Akbar 7.0822\n",
            "Islam 7.035\n",
            "Sheikh 7.0305\n",
            "member 6.9709\n",
            "as 6.8522\n",
            "king 6.8267\n",
            "Ibrahim 6.7738\n",
            "Omar 6.6273\n",
            "Sultan 6.5135\n",
            "********* Closest sentence neighbors in output to the token :   ##r\n",
            "is 3.5696\n",
            "Abu 3.0305\n",
            "Sid -0.8519\n",
            "\n",
            "\n",
            "********* Top predictions for token:  Sid\n",
            "Sid 25.8105\n",
            "Sin 13.9732\n",
            "Sad 13.9711\n",
            "In 13.5312\n",
            "Bud 13.0095\n",
            "Se 11.6849\n",
            "Abu 11.5092\n",
            "in 11.0562\n",
            "Mad 10.5109\n",
            "Said 10.4935\n",
            "Ad 10.4623\n",
            "Had 10.3935\n",
            "Mu 10.3643\n",
            "Sun 10.2971\n",
            "Bin 10.1099\n",
            "Man 9.9245\n",
            "Mo 9.7569\n",
            "Na 9.6466\n",
            "Mar 9.6304\n",
            "Sul 9.5607\n",
            "Sub 9.3739\n",
            "********* Closest sentence neighbors in output to the token :   Sid\n",
            "Sid 25.8105\n",
            "Abu 11.5092\n",
            "is 2.6309\n",
            "\n",
            "\n",
            "********* Top predictions for token:  ##di\n",
            "Di 12.2162\n",
            "di 9.604\n",
            "only 7.8545\n",
            "De 7.651\n",
            "that 7.5076\n",
            "Ta 7.4381\n",
            "Courtney 7.4342\n",
            "of 7.2446\n",
            "Ra 6.9299\n",
            "ta 6.9193\n",
            "when 6.9173\n",
            "Yi 6.7714\n",
            "suppression 6.67\n",
            "plain 6.454\n",
            "to 6.4506\n",
            "ma 6.3984\n",
            "Sunni 6.2783\n",
            "anti 6.2544\n",
            "with 6.2325\n",
            "Sky 6.228\n",
            "da 6.2222\n",
            "********* Closest sentence neighbors in output to the token :   ##di\n",
            "is 3.3127\n",
            "Sid 1.0182\n",
            "Abu -0.1357\n",
            "\n",
            "\n",
            "********* Top predictions for token:  ##q\n",
            "Malik 10.6791\n",
            "Muhammad 9.6467\n",
            "Khalid 9.0724\n",
            "Salem 9.0484\n",
            "Dubai 8.2997\n",
            "Mohammad 8.222\n",
            "Muslim 7.8685\n",
            "Mughal 7.7876\n",
            "Islam 7.772\n",
            "and 7.7714\n",
            "Allah 7.4853\n",
            "Rahman 7.4489\n",
            "Qi 7.439\n",
            "Akbar 7.3089\n",
            "lot 7.3084\n",
            "Ali 7.0197\n",
            "Quran 6.921\n",
            "Ahmed 6.6615\n",
            "Sharif 6.6097\n",
            "Ibrahim 6.5936\n",
            "Bengal 6.472\n",
            "********* Closest sentence neighbors in output to the token :   ##q\n",
            "is 3.905\n",
            "Abu -0.353\n",
            "Sid -6.151\n",
            "\n",
            "\n",
            "********* Top predictions for token:  is\n",
            "is 15.6773\n",
            "was 12.0303\n",
            "of 9.707\n",
            "and 9.4068\n",
            "in 8.9827\n",
            "as 7.6807\n",
            "Is 7.4987\n",
            "Muhammad 7.4649\n",
            "isn 7.4056\n",
            "for 7.4016\n",
            "bin 7.3627\n",
            "at 7.2015\n",
            "or 7.1999\n",
            "to 7.1234\n",
            "by 7.0612\n",
            "has 7.0293\n",
            "are 6.8039\n",
            "from 6.764\n",
            "born 6.4809\n",
            "died 6.2447\n",
            "also 6.2102\n",
            "********* Closest sentence neighbors in output to the token :   is\n",
            "is 15.6773\n",
            "Abu 4.8995\n",
            "Sid -1.2634\n",
            "\n",
            "\n",
            "********* Top predictions for token:  a\n",
            "an 9.8332\n",
            "born 8.8242\n",
            "the 8.5351\n",
            "one 8.1131\n",
            "in 7.432\n",
            "another 7.4147\n",
            "his 7.1203\n",
            "is 6.5993\n",
            "of 6.595\n",
            "any 6.559\n",
            "from 6.4254\n",
            "former 6.2992\n",
            "no 6.2922\n",
            "poet 6.2151\n",
            "author 6.1663\n",
            "some 6.1313\n",
            "known 6.0838\n",
            "actor 6.0461\n",
            "politician 5.7814\n",
            "at 5.7548\n",
            "this 5.7439\n",
            "********* Closest sentence neighbors in output to the token :   a\n",
            "is 6.5993\n",
            "Abu 5.41\n",
            "Sid -1.0469\n",
            "\n",
            "\n",
            "********* Top predictions for token:  [MASK]\n",
            "politician 7.3762\n",
            "Muslim 7.2441\n",
            "Pakistani 7.034\n",
            "Nigerian 6.9657\n",
            "poet 6.8297\n",
            "cricketer 6.7642\n",
            "Bangladeshi 6.6879\n",
            "writer 6.5243\n",
            "Syrian 6.4968\n",
            "Palestinian 6.3825\n",
            "lawyer 6.2746\n",
            "blind 6.2323\n",
            "musician 6.0834\n",
            "businessman 6.0588\n",
            "mathematician 5.9707\n",
            "scholar 5.9668\n",
            "terrorist 5.8992\n",
            "singer 5.8056\n",
            "judge 5.731\n",
            "boxer 5.5955\n",
            "of 5.5939\n",
            "********* Closest sentence neighbors in output to the token :   [MASK]\n",
            "Abu 3.4592\n",
            "is 1.7748\n",
            "Sid -0.8658\n",
            "\n",
            "\n",
            "********* Top predictions for token:  [SEP]\n",
            "of 7.3392\n",
            "to 7.0953\n",
            "and 6.5986\n",
            "or 6.4131\n",
            "who 6.1599\n",
            "which 6.0056\n",
            "in 5.9439\n",
            "forward 5.9078\n",
            "than 5.8394\n",
            "one 5.5551\n",
            "but 5.5546\n",
            "as 5.3159\n",
            "the 5.2993\n",
            "is 5.2954\n",
            "Abu 5.2875\n",
            "that 5.2764\n",
            "for 5.1829\n",
            "an 4.9243\n",
            "Baba 4.8657\n",
            "2010 4.7975\n",
            "however 4.7677\n",
            "********* Closest sentence neighbors in output to the token :   [SEP]\n",
            "is 5.2954\n",
            "Abu 5.2875\n",
            "Sid -3.6463\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "test_notebook.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}