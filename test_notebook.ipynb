{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Set up environment in step below**"
      ],
      "metadata": {
        "id": "HBM7lKLZG3uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0 transformers==4.19.1 sacremoses==0.0.53"
      ],
      "metadata": {
        "id": "m0RsXkue3W-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Run the cell below once**"
      ],
      "metadata": {
        "id": "jFIU70yGHDia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import traceback\n",
        "import argparse\n",
        "import string\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "#DEFAULT_MODEL_PATH='bert-large-cased'\n",
        "#DEFAULT_MODEL_PATH='bert-base-cased' #works best for names\n",
        "#DEFAULT_MODEL_PATH='bert-base-uncased'\n",
        "DEFAULT_MODEL_PATH='./'\n",
        "DEFAULT_TO_LOWER=False\n",
        "DEFAULT_TOP_K = 20\n",
        "ACCRUE_THRESHOLD = 1\n",
        "\n",
        "def init_model(model_path,to_lower):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    print(\"******* MODEL[path] is:\",model_path,\" lower casing is set to:\",to_lower)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
        "    model = BertForMaskedLM.from_pretrained(model_path)\n",
        "    #tokenizer = RobertaTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
        "    #model = RobertaForMaskedLM.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    return model,tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_task(model,tokenizer,top_k,accrue_threshold,text,patched):\n",
        "    text = '[CLS] ' + text + ' [SEP]' \n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # Create the segments tensors.\n",
        "    segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "    print(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(tokens_tensor, segments_tensors)\n",
        "        for i in range(len(tokenized_text)):\n",
        "                #if (i != 0 and i != len(tokenized_text) - 1):\n",
        "                #    continue\n",
        "                results_dict = {}\n",
        "                masked_index = i\n",
        "                neighs_dict = {}\n",
        "                if (patched):\n",
        "                    for j in range(len(predictions[0][0][0,masked_index])):\n",
        "                        if (float(predictions[0][0][0,masked_index][j].tolist()) > accrue_threshold):\n",
        "                            tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                            results_dict[tok] = float(predictions[0][0][0,masked_index][j].tolist())\n",
        "                        tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                        if (tok in tokenized_text):\n",
        "                            neighs_dict[tok] = float(predictions[0][0][0,masked_index][j].tolist())\n",
        "                else:\n",
        "                    for j in range(len(predictions[0][0][masked_index])):\n",
        "                        if (float(predictions[0][0][masked_index][j].tolist()) > accrue_threshold):\n",
        "                            tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                            results_dict[tok] = float(predictions[0][0][masked_index][j].tolist())\n",
        "                        tok = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                        if (tok in tokenized_text):\n",
        "                            neighs_dict[tok] = float(predictions[0][0][masked_index][j].tolist())\n",
        "                k = 0\n",
        "                sorted_d = OrderedDict(sorted(results_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                print(\"********* Top predictions for token: \",tokenized_text[i])\n",
        "                for index in sorted_d:\n",
        "                    if (index in string.punctuation or index.startswith('##') or len(index) == 1 or index.startswith('.') or index.startswith('[')):\n",
        "                        continue\n",
        "                    print(index,round(float(sorted_d[index]),4))\n",
        "                    k += 1\n",
        "                    if (k > top_k):\n",
        "                        break\n",
        "                print(\"********* Closest sentence neighbors in output to the token :  \",tokenized_text[i])\n",
        "                sorted_d = OrderedDict(sorted(neighs_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                for index in sorted_d:\n",
        "                    if (index in string.punctuation or index.startswith('##') or len(index) == 1 or index.startswith('.') or index.startswith('[')):\n",
        "                        continue\n",
        "                    print(index,round(float(sorted_d[index]),4))\n",
        "                print()\n",
        "                print()\n",
        "                #break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ufTuOUHi3KQO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Modify params below and execute for results**"
      ],
      "metadata": {
        "id": "VkrTX3mc7fik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = \"bert-base-cased\"\n",
        "topk = DEFAULT_TOP_K\n",
        "tolower=False\n",
        "patched = False\n",
        "threshold = ACCRUE_THRESHOLD\n",
        "model,tokenizer = init_model(model,tolower)\n",
        "text = \"John flew from New York to [MASK]\"\n",
        "perform_task(model,tokenizer,topk,threshold,text,patched)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6H8--aO7iIz",
        "outputId": "556e5f7a-0c9a-4689-a84c-b297ee863d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******* MODEL[path] is: bert-base-cased  lower casing is set to: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'John', 'flew', 'from', 'New', 'York', 'to', '[MASK]', '[SEP]']\n",
            "********* Top predictions for token:  [CLS]\n",
            "the 4.7987\n",
            "and 4.2494\n",
            "of 4.1367\n",
            "to 4.0774\n",
            "in 3.8398\n",
            "was 3.3081\n",
            "it 3.05\n",
            "her 3.0426\n",
            "him 3.0382\n",
            "that 2.9599\n",
            "on 2.9331\n",
            "for 2.8487\n",
            "as 2.7878\n",
            "his 2.7741\n",
            "families 2.7639\n",
            "with 2.7462\n",
            "is 2.7374\n",
            "me 2.7371\n",
            "The 2.6937\n",
            "he 2.6935\n",
            "from 2.6255\n",
            "********* Closest sentence neighbors in output to the token :   [CLS]\n",
            "to 4.0774\n",
            "from 2.6255\n",
            "York 1.2461\n",
            "New 0.9595\n",
            "John 0.464\n",
            "flew -1.9623\n",
            "\n",
            "\n",
            "********* Top predictions for token:  John\n",
            "the 5.5664\n",
            "and 5.0206\n",
            "of 4.8988\n",
            "to 4.6027\n",
            "in 4.449\n",
            "was 3.9431\n",
            "as 3.7773\n",
            "The 3.6805\n",
            "he 3.6125\n",
            "on 3.5829\n",
            "is 3.5353\n",
            "with 3.5117\n",
            "that 3.4356\n",
            "for 3.3814\n",
            "her 3.3356\n",
            "his 3.3321\n",
            "at 3.3007\n",
            "it 3.2901\n",
            "by 3.0797\n",
            "He 3.0736\n",
            "from 3.0672\n",
            "********* Closest sentence neighbors in output to the token :   John\n",
            "to 4.6027\n",
            "from 3.0672\n",
            "John 1.3122\n",
            "New 1.029\n",
            "York 0.77\n",
            "flew -2.4839\n",
            "\n",
            "\n",
            "********* Top predictions for token:  flew\n",
            "flew 17.0532\n",
            "flying 10.6339\n",
            "wrote 10.3467\n",
            "went 10.3463\n",
            "flown 10.2039\n",
            "was 10.1331\n",
            "flies 9.6411\n",
            "fly 9.5486\n",
            "drove 9.4184\n",
            "sailed 9.2796\n",
            "moved 8.5902\n",
            "landed 8.4895\n",
            "and 8.4578\n",
            "he 8.386\n",
            "took 8.3505\n",
            "came 8.2026\n",
            "to 8.1442\n",
            "Airport 8.0782\n",
            "called 7.8742\n",
            "reported 7.8018\n",
            "flight 7.7784\n",
            "********* Closest sentence neighbors in output to the token :   flew\n",
            "flew 17.0532\n",
            "to 8.1442\n",
            "from 7.0757\n",
            "John 5.3746\n",
            "York 1.5992\n",
            "New -1.6888\n",
            "\n",
            "\n",
            "********* Top predictions for token:  from\n",
            "from 26.8673\n",
            "From 15.1533\n",
            "to 14.6699\n",
            "of 11.3221\n",
            "the 10.8798\n",
            "in 10.8375\n",
            "with 10.442\n",
            "at 10.4307\n",
            "between 10.4077\n",
            "via 9.911\n",
            "and 9.3715\n",
            "through 9.3034\n",
            "into 8.6265\n",
            "around 8.5542\n",
            "by 8.4984\n",
            "over 8.1881\n",
            "about 8.118\n",
            "left 8.0817\n",
            "said 7.4225\n",
            "only 7.3774\n",
            "out 7.3192\n",
            "********* Closest sentence neighbors in output to the token :   from\n",
            "from 26.8673\n",
            "to 14.6699\n",
            "John 4.3833\n",
            "flew 4.2552\n",
            "New 4.0624\n",
            "York 0.8015\n",
            "\n",
            "\n",
            "********* Top predictions for token:  New\n",
            "New 21.1869\n",
            "NY 13.7142\n",
            "York 12.8125\n",
            "Brooklyn 10.8073\n",
            "United 10.5354\n",
            "Washington 9.867\n",
            "and 9.6684\n",
            "San 9.3435\n",
            "Buffalo 9.3364\n",
            "Manhattan 9.1193\n",
            "new 8.8794\n",
            "Albany 8.8457\n",
            "Boston 8.8133\n",
            "don 8.5572\n",
            "the 8.5391\n",
            "Newark 8.3896\n",
            "City 8.3325\n",
            "London 8.2444\n",
            "as 8.1362\n",
            "Harlem 8.1336\n",
            "Philadelphia 7.8448\n",
            "********* Closest sentence neighbors in output to the token :   New\n",
            "New 21.1869\n",
            "York 12.8125\n",
            "to 7.7493\n",
            "John 5.703\n",
            "from 4.9422\n",
            "flew -2.1477\n",
            "\n",
            "\n",
            "********* Top predictions for token:  York\n",
            "York 26.0595\n",
            "NY 15.0005\n",
            "Jersey 12.4364\n",
            "London 11.0917\n",
            "Albany 10.4183\n",
            "Amsterdam 10.1513\n",
            "up 10.0225\n",
            "England 9.6956\n",
            "Mexico 9.5321\n",
            "Yorker 8.9052\n",
            "Buffalo 8.8369\n",
            "Haven 8.4718\n",
            "Hudson 8.236\n",
            "Lincoln 7.9297\n",
            "Manhattan 7.8328\n",
            "Boston 7.7514\n",
            "Brooklyn 7.7396\n",
            "Orleans 7.7176\n",
            "Vegas 7.6707\n",
            "down 7.5942\n",
            "by 7.5867\n",
            "********* Closest sentence neighbors in output to the token :   York\n",
            "York 26.0595\n",
            "to 6.1489\n",
            "New 3.1722\n",
            "John 2.9022\n",
            "from 2.6229\n",
            "flew -0.6285\n",
            "\n",
            "\n",
            "********* Top predictions for token:  to\n",
            "to 21.9062\n",
            "and 13.1749\n",
            "on 10.6828\n",
            "into 9.7422\n",
            "To 9.6542\n",
            "with 9.4552\n",
            "the 9.2271\n",
            "in 9.0718\n",
            "from 8.8974\n",
            "for 8.7988\n",
            "of 8.7369\n",
            "there 8.5647\n",
            "that 8.4781\n",
            "at 8.3531\n",
            "via 8.3463\n",
            "through 8.1372\n",
            "it 8.0436\n",
            "toward 7.8465\n",
            "an 7.6464\n",
            "towards 7.5334\n",
            "onto 7.5166\n",
            "********* Closest sentence neighbors in output to the token :   to\n",
            "to 21.9062\n",
            "from 8.8974\n",
            "John 3.6396\n",
            "York 2.7377\n",
            "New 1.9581\n",
            "flew 1.4184\n",
            "\n",
            "\n",
            "********* Top predictions for token:  [MASK]\n",
            "London 7.3445\n",
            "Boston 6.7979\n",
            "Chicago 6.5728\n",
            "England 6.5351\n",
            "Washington 6.4742\n",
            "California 6.2737\n",
            "Mexico 6.2624\n",
            "Paris 6.2472\n",
            "Rome 6.1924\n",
            "Philadelphia 6.086\n",
            "France 6.0245\n",
            "Seattle 5.8558\n",
            "Canada 5.8137\n",
            "Australia 5.7744\n",
            "Melbourne 5.7597\n",
            "China 5.6754\n",
            "Virginia 5.6506\n",
            "Vietnam 5.6331\n",
            "Italy 5.5312\n",
            "Japan 5.4845\n",
            "Brazil 5.4214\n",
            "********* Closest sentence neighbors in output to the token :   [MASK]\n",
            "York 4.8144\n",
            "New 2.4068\n",
            "John 2.3974\n",
            "to 2.0144\n",
            "from 0.7201\n",
            "flew -2.9866\n",
            "\n",
            "\n",
            "********* Top predictions for token:  [SEP]\n",
            "of 6.9259\n",
            "to 5.9796\n",
            "was 5.9391\n",
            "he 5.6976\n",
            "He 5.6317\n",
            "and 5.469\n",
            "the 5.4503\n",
            "she 5.1847\n",
            "in 5.172\n",
            "She 5.1653\n",
            "for 5.1521\n",
            "on 4.8495\n",
            "her 4.8314\n",
            "had 4.6283\n",
            "his 4.5603\n",
            "by 4.5166\n",
            "that 4.4776\n",
            "be 4.4067\n",
            "is 4.3083\n",
            "an 4.2108\n",
            "it 4.1587\n",
            "********* Closest sentence neighbors in output to the token :   [SEP]\n",
            "to 5.9796\n",
            "from 3.9703\n",
            "York 3.8934\n",
            "John 3.0994\n",
            "New 2.7605\n",
            "flew -2.3547\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "test_notebook.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}